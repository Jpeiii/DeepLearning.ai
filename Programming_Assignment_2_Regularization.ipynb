{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Programming Assignment 2: Regularization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Jpeiii/DeepLearning.ai/blob/master/Programming_Assignment_2_Regularization.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "yi6nrdOXBEUg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Problem Statement: **\n",
        "\n",
        "You have just been hired as an AI expert by the French Football Corporation. They would like you to recommend positions where France's goal keeper should kick the ball so that the French team's players can then hit it with their head.\n",
        "\n",
        "**Your goal: **\n",
        "\n",
        "Use a deep learning model to find the positions on the field where the goalkeeper should kick the ball.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "boy_6OozB5kF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Regularization:**\n",
        "\n",
        "\n",
        "\n",
        "1.   Non-regularized model\n",
        "2.   L2 Regularization\n",
        "3.   Dropout\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "O-_Ui59ICbjF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**1. Non-regularized model**"
      ]
    },
    {
      "metadata": {
        "id": "bfLW5WVYCfjc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "\n",
        "def load_2D_dataset():\n",
        "\n",
        "    data = scipy.io.loadmat('C:/Users/DELL/Desktop/A.I/Deep learning.data/Assignment 7')\n",
        "    train_X = data['X'].T\n",
        "    train_Y = data['y'].T\n",
        "    test_X = data['Xval'].T\n",
        "    test_Y = data['yval'].T\n",
        "\n",
        "    plt.scatter(train_X[0, :], train_X[1, :], c=np.squeeze(train_Y), s=40, cmap=plt.cm.Spectral);\n",
        "\n",
        "    return train_X, train_Y, test_X, test_Y\n",
        "\n",
        "train_X, train_Y, test_X, test_Y = load_2D_dataset()\n",
        "\n",
        "\n",
        "def sigmoid(Z):\n",
        "\n",
        "    A = 1/(1 + np.exp(-Z))\n",
        "\n",
        "    return A\n",
        "\n",
        "def relu(Z):\n",
        "\n",
        "    A = np.maximum(0, Z)\n",
        "\n",
        "    return A\n",
        "\n",
        "def initialize_parameters_random(layer_dims):\n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) / np.sqrt(layer_dims[l - 1])\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "        assert (parameters['W' + str(l)].shape == layer_dims[l], layer_dims[l - 1])\n",
        "        assert (parameters['W' + str(l)].shape == layer_dims[l], 1)\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def compute_cost(A3, Y):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "    logprobs = np.multiply(-np.log(A3), Y) + np.multiply(-np.log(1-A3), (1-Y))\n",
        "    cost = 1/m* np.nansum(logprobs)\n",
        "\n",
        "    return cost\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    W3 = parameters['W3']\n",
        "    b3 = parameters['b3']\n",
        "\n",
        "\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = relu(Z2)\n",
        "    Z3 = np.dot(W3, A2) + b3\n",
        "    A3 = sigmoid(Z3)\n",
        "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
        "\n",
        "\n",
        "    return A3, cache\n",
        "\n",
        "def backward_propagation(X, Y, cache):\n",
        "\n",
        "    m = X.shape[1]\n",
        "\n",
        "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
        "\n",
        "    dz3 = 1/m * (A3 - Y)\n",
        "    dW3 = np.dot(dz3, A2.T)\n",
        "    db3 = np.sum(dz3, axis=1, keepdims=True)\n",
        "\n",
        "    da2 = np.dot(W3.T, dz3)\n",
        "    dz2 = np.multiply(da2, np.int64(A2 > 0))\n",
        "    dW2 = np.dot(dz2, A1.T)\n",
        "    db2 = np.sum(dz2, axis=1, keepdims=True)\n",
        "\n",
        "    da1 = np.dot(W2.T, dz2)\n",
        "    dz1 = np.multiply(da1, np.int64(A1 > 0))\n",
        "    dW1 = np.dot(dz1, X.T)\n",
        "    db1 = np.sum(dz1, axis=1, keepdims=True)\n",
        "\n",
        "    gradients = {'dz3': dz3,\n",
        "                 'dW3': dW3,\n",
        "                 'db3': db3,\n",
        "                 'da2': da2,\n",
        "                 'dz2': dz2,\n",
        "                 'dW2': dW2,\n",
        "                 'db2': db2,\n",
        "                 'da1': da1,\n",
        "                 'dz1': dz1,\n",
        "                 'dW1': dW1,\n",
        "                 'db1': db1}\n",
        "\n",
        "    return gradients\n",
        "\n",
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "\n",
        "    L = len(parameters)//2\n",
        "\n",
        "    for k in range(L):\n",
        "        parameters['W' + str(k+1)] = parameters['W' + str(k+1)] - learning_rate * gradients['dW' + str(k+1)]\n",
        "        parameters['b' + str(k+1)] = parameters['b' + str(k+1)] - learning_rate * gradients['db' + str(k+1)]\n",
        "\n",
        "    return parameters\n",
        "\n",
        "def predict(X,Y, parameters):\n",
        "\n",
        "    m = X.shape[1]\n",
        "    p = np.zeros((1, m), dtype=np.int)\n",
        "\n",
        "    A3, cache =  forward_propagation(X, parameters)\n",
        "\n",
        "    for i in range(0, A3.shape[1]):\n",
        "        if A3[0, i] > 0.5:\n",
        "            p[0, i] = 1\n",
        "        else:\n",
        "            p[0, i] = 0\n",
        "\n",
        "    print('Accuracy:' + str(np.mean((p[0,:] == Y[0,:]))))\n",
        "\n",
        "    return p\n",
        "\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    # Set min and max values and give it some padding\n",
        "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
        "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
        "    h = 0.01\n",
        "    # Generate a grid of points with distance h between them\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    # Predict the function value for the whole grid\n",
        "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    # Plot the contour and training examples\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
        "    plt.ylabel('x2')\n",
        "    plt.xlabel('x1')\n",
        "    plt.scatter(X[0, :], X[1, :], c=np.squeeze(y), cmap=plt.cm.Spectral)\n",
        "    plt.show()\n",
        "\n",
        "def predict_decision(parameters, X):\n",
        "    # used for plotting decision boundary\n",
        "\n",
        "    A3, cache =  forward_propagation(X, parameters)\n",
        "    predictions = (A3 > 0.5)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "def model(X, Y, learning_rate=0.3, num_iterations=30000, print_cost=True):\n",
        "    gradients = {}\n",
        "    costs = []\n",
        "    m = X.shape[1]\n",
        "    layer_dims = [X.shape[0], 20, 3, 1]\n",
        "    parameters = initialize_parameters_random(layer_dims)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        A3, cache = forward_propagation(X, parameters)\n",
        "        cost = compute_cost(A3, Y)\n",
        "        gradients = backward_propagation(X, Y, cache)\n",
        "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "        if print_cost and i % 10000 == 0:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
        "            costs.append(cost)\n",
        "\n",
        "    plt.plot(costs)\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "parameters = model(train_X, train_Y)\n",
        "\n",
        "print (\"On the training set:\")\n",
        "predictions_train = predict(train_X, train_Y, parameters)\n",
        "print (\"On the test set:\")\n",
        "predictions_test = predict(test_X, test_Y, parameters)\n",
        "\n",
        "plt.title(\"Model without regularization\")\n",
        "axes = plt.gca()\n",
        "axes.set_xlim([-0.75,0.40])\n",
        "axes.set_ylim([-0.75,0.65])\n",
        "plot_decision_boundary(lambda x: predict_decision(parameters, x.T), train_X, train_Y)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "79wTxA0qDABZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**2. L2 Regularization**"
      ]
    },
    {
      "metadata": {
        "id": "_Z1TXyo9DIfy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "\n",
        "def load_2D_dataset():\n",
        "\n",
        "    data = scipy.io.loadmat('C:/Users/DELL/Desktop/A.I/Deep learning.data/Assignment 7')\n",
        "    train_X = data['X'].T\n",
        "    train_Y = data['y'].T\n",
        "    test_X = data['Xval'].T\n",
        "    test_Y = data['yval'].T\n",
        "\n",
        "    #plt.scatter(train_X[0, :], train_X[1, :], c=np.squeeze(train_Y), s=40, cmap=plt.cm.Spectral);\n",
        "\n",
        "    return train_X, train_Y, test_X, test_Y\n",
        "\n",
        "train_X, train_Y, test_X, test_Y = load_2D_dataset()\n",
        "\n",
        "def sigmoid(x):\n",
        "\n",
        "    s = 1/(1 + np.exp(-x))\n",
        "\n",
        "    return s\n",
        "\n",
        "def relu(x):\n",
        "\n",
        "    s = np.maximum(0,x)\n",
        "\n",
        "    return s\n",
        "\n",
        "def initialize_parameters(layer_dims):\n",
        "\n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.rand(layer_dims[l], layer_dims[l-1])/np.sqrt(layer_dims[l-1])\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "    return parameters\n",
        "\n",
        "def compute_cost(A3, Y):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "    logprobs = np.multiply(-np.log(A3), Y) + np.multiply(-np.log(1-A3), (1-Y))\n",
        "    cost = 1/m* np.nansum(logprobs)\n",
        "\n",
        "    return cost\n",
        "\n",
        "def compute_cost_with_regularization(A3, Y, parameters, lambd):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "    W3 = parameters['W3']\n",
        "\n",
        "    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross entropy part of the cost\n",
        "\n",
        "    L2_regularization_cost = (1/m*lambd/2)*(np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\n",
        "\n",
        "    cost = cross_entropy_cost + L2_regularization_cost\n",
        "\n",
        "    return cost\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    W3 = parameters['W3']\n",
        "    b3 = parameters['b3']\n",
        "\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = relu(Z2)\n",
        "    Z3 = np.dot(W3, A2) + b3\n",
        "    A3 = sigmoid(Z3)\n",
        "\n",
        "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
        "\n",
        "    return A3, cache\n",
        "\n",
        "def backward_propagation_with_regularization(X, Y, cache, lambd):\n",
        "\n",
        "    m = X.shape[1]\n",
        "\n",
        "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
        "\n",
        "    dz3 = A3 - Y\n",
        "    dw3 = 1/m * np.dot(dz3, A2.T) + lambd/m *W3\n",
        "    db3 = 1/m * np.sum(dz3, axis=1, keepdims=True)\n",
        "\n",
        "    da2 = np.dot(W3.T, dz3)\n",
        "    dz2 = np.multiply(da2, np.int64(A2 > 0))\n",
        "    dw2 = 1/m * np.dot(dz2, A1.T) + lambd/m *W2\n",
        "    db2 = 1/m * np.sum(dz2, axis=1, keepdims=True)\n",
        "\n",
        "    da1 = np.dot(W2.T, dz2)\n",
        "    dz1 = np.multiply(da1, np.int64(A1 > 0))\n",
        "    dw1 = 1/m * np.dot(dz1, X.T) + lambd/m *W1\n",
        "    db1 = 1/m * np.sum(dz1, axis=1, keepdims=True)\n",
        "\n",
        "    gradients = {'dz3' :dz3,\n",
        "                 'dw3' :dw3,\n",
        "                 'db3' :db3,\n",
        "                 'dz2' :dz2,\n",
        "                 'dw2' :dw2,\n",
        "                 'db2' :db2,\n",
        "                 'dz1' :dz1,\n",
        "                 'db1' :db1,\n",
        "                 'dw1' :dw1}\n",
        "\n",
        "    return gradients\n",
        "\n",
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "\n",
        "    L = len(parameters)//2\n",
        "    for k in range(L):\n",
        "        parameters['W' + str(k+1)] = parameters['W' + str(k+1)] - learning_rate * gradients['dw' + str(k+1)]\n",
        "        parameters['b' + str(k+1)] = parameters['b' + str(k+1)] - learning_rate * gradients['db' + str(k+1)]\n",
        "\n",
        "    return parameters\n",
        "\n",
        "def predict(X, Y, parameters):\n",
        "\n",
        "    m = X.shape[1]\n",
        "    p = np.zeros((1,m), dtype=np.int)\n",
        "\n",
        "    A3, cache = forward_propagation(X, parameters)\n",
        "\n",
        "    for i in range(0, A3.shape[1]):\n",
        "        if A3[0,i] > 0.5:\n",
        "            p[0,i] = 1\n",
        "        else:\n",
        "            p[0,i] = 0\n",
        "\n",
        "    print('Accuracy:' + str(np.mean((p[0,:] == Y[0,:]))))\n",
        "\n",
        "    return p\n",
        "\n",
        "def model(X, Y, learning_rate=0.3, num_iterations=30000, print_cost=True, lambd=0.7):\n",
        "\n",
        "    gradients = {}\n",
        "    costs = []\n",
        "    m = X.shape[1]\n",
        "    layer_dims = [X.shape[0], 20, 3, 1]\n",
        "    parameters = initialize_parameters(layer_dims)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "        A3, cache = forward_propagation(X, parameters)\n",
        "        cost = compute_cost_with_regularization(A3, Y, parameters, lambd)\n",
        "        gradients = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
        "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "        if print_cost and i % 10000 == 0:\n",
        "            print('Cost after iteration{}:{}'.format(i, cost))\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "\n",
        "    plt.plot(costs)\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (x1,000)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    return parameters\n",
        "\n",
        "parameters = model(train_X, train_Y, learning_rate=0.3, num_iterations=30000, print_cost=True, lambd=0.7)\n",
        "\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    # Set min and max values and give it some padding\n",
        "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
        "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
        "    h = 0.01\n",
        "    # Generate a grid of points with distance h between them\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    # Predict the function value for the whole grid\n",
        "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    # Plot the contour and training examples\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
        "    plt.ylabel('x2')\n",
        "    plt.xlabel('x1')\n",
        "    plt.scatter(X[0, :], X[1, :], c=np.squeeze(y), cmap=plt.cm.Spectral)\n",
        "    plt.show()\n",
        "\n",
        "def predict_decision(parameters, X):\n",
        "    # used for plotting decision boundary\n",
        "\n",
        "    A3, cache =  forward_propagation()X, parameters)\n",
        "    predictions = (A3 > 0.5)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "print (\"On the training set:\")\n",
        "predictions_train = predict(train_X, train_Y, parameters)\n",
        "print (\"On the test set:\")\n",
        "predictions_test = predict(test_X, test_Y, parameters)\n",
        "\n",
        "plt.title(\"Model without regularization\")\n",
        "axes = plt.gca()\n",
        "axes.set_xlim([-0.75,0.40])\n",
        "axes.set_ylim([-0.75,0.65])\n",
        "plot_decision_boundary(lambda x: predict_decision(parameters, x.T), train_X, train_Y)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l1sqbKefDTmi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**3. Dropout**"
      ]
    },
    {
      "metadata": {
        "id": "2hiJgg2XDaZa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "\n",
        "def load_2D_dataset():\n",
        "\n",
        "    data = scipy.io.loadmat('C:/Users/DELL/Desktop/A.I/Deep learning.data/Assignment 7')\n",
        "    train_X = data['X'].T\n",
        "    train_Y = data['y'].T\n",
        "    test_X = data['Xval'].T\n",
        "    test_Y = data['yval'].T\n",
        "\n",
        "    #plt.scatter(train_X[0, :], train_X[1, :], c=np.squeeze(train_Y), s=40, cmap=plt.cm.Spectral);\n",
        "\n",
        "    return train_X, train_Y, test_X, test_Y\n",
        "\n",
        "train_X, train_Y, test_X, test_Y = load_2D_dataset()\n",
        "\n",
        "def sigmoid(x):\n",
        "\n",
        "    s = 1/(1 + np.exp(-x))\n",
        "\n",
        "    return s\n",
        "\n",
        "def relu(x):\n",
        "\n",
        "    s = np.maximum(0,x)\n",
        "\n",
        "    return s\n",
        "\n",
        "def initialize_parameters(layer_dims):\n",
        "\n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "\n",
        "    return parameters\n",
        "\n",
        "def compute_cost(A3, Y):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "    logprobs = np.multiply(-np.log(A3), Y) + np.multiply(np.log(1 - A3), (1-Y))\n",
        "    cost = 1/m * np.nansum(logprobs)\n",
        "\n",
        "    return cost\n",
        "\n",
        "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n",
        "\n",
        "    np.random.seed(1)\n",
        "\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    W3 = parameters[\"W3\"]\n",
        "    b3 = parameters[\"b3\"]\n",
        "\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = relu(Z1)\n",
        "    D1 = np.random.randn(A1.shape[0], A1.shape[1])\n",
        "    D1 = D1 < keep_prob\n",
        "    A1 = A1 * D1\n",
        "    A1 = A1 / keep_prob\n",
        "\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = relu(Z2)\n",
        "    D2 = np.random.randn(A2.shape[0], A2.shape[1])\n",
        "    D2 = D2 < keep_prob\n",
        "    A2 = A2 * D2\n",
        "    A2 = A2 / keep_prob\n",
        "\n",
        "    Z3 = np.dot(W3, A2) + b3\n",
        "    A3 = sigmoid(Z3)\n",
        "\n",
        "    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
        "\n",
        "    return A3, cache\n",
        "\n",
        "def backward_propagation_with_dropout(X, Y, cache, keep_prob):\n",
        "\n",
        "    m = X.shape[1]\n",
        "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
        "\n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
        "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
        "    dA2 = np.dot(W3.T, dZ3)\n",
        "    dA2 = dA2 * D2\n",
        "    dA2 = dA2 / keep_prob\n",
        "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
        "    dW2 = 1./m * np.dot(dZ2, A1.T)\n",
        "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
        "    dA1 = np.dot(W2.T, dZ2)\n",
        "    dA1 = dA1 * D1\n",
        "    dA1 = dA1 / keep_prob\n",
        "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
        "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
        "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
        "\n",
        "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
        "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1,\n",
        "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
        "\n",
        "    return gradients\n",
        "\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "\n",
        "    L = len(parameters) // 2\n",
        "\n",
        "\n",
        "    for k in range(L):\n",
        "\n",
        "        parameters[\"W\" + str(k+1)] = parameters[\"W\" + str(k+1)] - learning_rate * grads[\"dW\" + str(k+1)]\n",
        "        parameters[\"b\" + str(k+1)] = parameters[\"b\" + str(k+1)] - learning_rate * grads[\"db\" + str(k+1)]\n",
        "\n",
        "    return parameters\n",
        "\n",
        "def predict(X, Y, parameters):\n",
        "\n",
        "    m = X.shape[1]\n",
        "    p = np.zeros((1,m), dtype=np.int)\n",
        "\n",
        "    A3, cache = forward_propagation_with_dropout(X, parameters)\n",
        "\n",
        "    for i in range(0, A3.shape[1]):\n",
        "        if A3[0,i] > 0.5:\n",
        "            p[0,i] = 1\n",
        "        else:\n",
        "            p[0,i] = 0\n",
        "\n",
        "    print('Accuracy:' + str(np.mean((p[0,:] == Y[0,:]))))\n",
        "\n",
        "    return p\n",
        "\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    # Set min and max values and give it some padding\n",
        "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
        "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
        "    h = 0.01\n",
        "    # Generate a grid of points with distance h between them\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    # Predict the function value for the whole grid\n",
        "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    # Plot the contour and training examples\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
        "    plt.ylabel('x2')\n",
        "    plt.xlabel('x1')\n",
        "    plt.scatter(X[0, :], X[1, :], c=np.squeeze(y), cmap=plt.cm.Spectral)\n",
        "    plt.show()\n",
        "\n",
        "def predict_decision(parameters, X):\n",
        "    # used for plotting decision boundary\n",
        "\n",
        "    A3, cache =  forward_propagation_with_dropout(X, parameters)\n",
        "    predictions = (A3 > 0.5)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, keep_prob = 1):\n",
        "\n",
        "\n",
        "    grads = {}\n",
        "    costs = []\n",
        "    m = X.shape[1]\n",
        "    layers_dims = [X.shape[0], 20, 3, 1]\n",
        "\n",
        "    parameters = initialize_parameters(layers_dims)\n",
        "\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "        A3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
        "        cost = compute_cost(A3, Y)\n",
        "        grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        # Print the loss every 10000 iterations\n",
        "        if print_cost and i % 10000 == 0:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "\n",
        "    plt.plot(costs)\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (x1,000)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "parameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)\n",
        "\n",
        "print (\"On the train set:\")\n",
        "predictions_train = predict(train_X, train_Y, parameters)\n",
        "print (\"On the test set:\")\n",
        "predictions_test = predict(test_X, test_Y, parameters)\n",
        "\n",
        "plt.title(\"Model with dropout\")\n",
        "axes = plt.gca()\n",
        "axes.set_xlim([-0.75,0.40])\n",
        "axes.set_ylim([-0.75,0.65])\n",
        "plot_decision_boundary(lambda x:  predict_decision(parameters, x.T), train_X, train_Y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}